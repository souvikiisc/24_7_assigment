{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-02T10:45:57.190126Z","iopub.status.busy":"2023-10-02T10:45:57.189423Z","iopub.status.idle":"2023-10-02T10:46:00.239454Z","shell.execute_reply":"2023-10-02T10:46:00.238020Z","shell.execute_reply.started":"2023-10-02T10:45:57.190091Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","import torch.optim as optim\n","from torch.autograd import Variable\n","from collections import Counter, OrderedDict"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-02T10:46:00.242155Z","iopub.status.busy":"2023-10-02T10:46:00.241145Z","iopub.status.idle":"2023-10-02T10:46:01.553123Z","shell.execute_reply":"2023-10-02T10:46:01.552139Z","shell.execute_reply.started":"2023-10-02T10:46:00.242112Z"},"trusted":true},"outputs":[],"source":["from scipy.spatial.distance import cosine\n","from torch.utils.data import DataLoader\n","from torchtext.data import to_map_style_dataset\n","from torchtext.data.utils import get_tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-02T10:46:01.555886Z","iopub.status.busy":"2023-10-02T10:46:01.555485Z","iopub.status.idle":"2023-10-02T10:46:01.560316Z","shell.execute_reply":"2023-10-02T10:46:01.559228Z","shell.execute_reply.started":"2023-10-02T10:46:01.555862Z"},"trusted":true},"outputs":[],"source":["from tqdm.auto import tqdm\n","from dataclasses import dataclass\n","import re\n","from random import random\n","import random as random"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-02T10:46:01.562442Z","iopub.status.busy":"2023-10-02T10:46:01.561796Z","iopub.status.idle":"2023-10-02T10:46:01.572648Z","shell.execute_reply":"2023-10-02T10:46:01.571751Z","shell.execute_reply.started":"2023-10-02T10:46:01.562411Z"},"trusted":true},"outputs":[],"source":["files = ['interview_ds.txt', 'interview_ds_2.txt']\n","\n","path = '/kaggle/input/text-247/'"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-02T10:46:01.575123Z","iopub.status.busy":"2023-10-02T10:46:01.574027Z","iopub.status.idle":"2023-10-02T10:46:01.584884Z","shell.execute_reply":"2023-10-02T10:46:01.584010Z","shell.execute_reply.started":"2023-10-02T10:46:01.575087Z"},"trusted":true},"outputs":[],"source":["def get_data():\n","    with open(f\"{path}{files[0]}\", 'r') as f:\n","        file1 = f.readlines()\n","\n","    with open(f\"{path}{files[1]}\", 'r') as f:\n","        file2 = f.readlines()\n","\n","    final_corpus = file1 + file2\n","    index = int(0.9*len(final_corpus))\n","    train_iter , valid_iter= final_corpus[:index], final_corpus[index:]\n","    return train_iter, valid_iter"]},{"cell_type":"markdown","metadata":{},"source":["# Define Model and Training Parameters here "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-02T10:46:01.587058Z","iopub.status.busy":"2023-10-02T10:46:01.586358Z","iopub.status.idle":"2023-10-02T10:46:01.628741Z","shell.execute_reply":"2023-10-02T10:46:01.627849Z","shell.execute_reply.started":"2023-10-02T10:46:01.586972Z"},"trusted":true},"outputs":[],"source":["@dataclass\n","class Word2VecParams:\n","\n","    # skipgram parameters\n","    MIN_FREQ = 50\n","    SKIPGRAM_N_WORDS = 7\n","    T = 1e-3\n","    NEG_SAMPLES = 10\n","    NS_ARRAY_LEN = 5_000_000\n","    SPECIALS = \"\"\n","    TOKENIZER = 'basic_english'\n","\n","    # network parameters\n","    BATCH_SIZE = 128\n","    EMBED_DIM = 64\n","    EMBED_MAX_NORM = None\n","    N_EPOCHS = 10\n","    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    CRITERION = nn.BCEWithLogitsLoss()"]},{"cell_type":"markdown","metadata":{},"source":["# Vocab Class"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-02T10:46:01.631112Z","iopub.status.busy":"2023-10-02T10:46:01.630217Z","iopub.status.idle":"2023-10-02T10:46:01.646571Z","shell.execute_reply":"2023-10-02T10:46:01.645609Z","shell.execute_reply.started":"2023-10-02T10:46:01.631079Z"},"trusted":true},"outputs":[],"source":["from torch._C import Value\n","\n","\n","###This class generates the vocabulary and provides helper function to get words from indexes and vice versa\n","\n","class Vocab:\n","\n","    def __init__(self, tokens, specials):\n","        self.stoi = {v[0] :(k, v[1]) for k,v in enumerate(tokens)}\n","        self.itos = {k:(v[0], v[1]) for k,v in enumerate(tokens)}\n","\n","        self._specials = specials[0]\n","        self.total_tokens = np.nansum([f for _, (_, f) in self.stoi.items()], dtype=int)\n","\n","\n","    def __len__(self):\n","        return len(self.stoi) - 1\n","\n","    def get_index(self, word):\n","\n","        if word in self.stoi:\n","            return self.stoi.get(word)[0]\n","        else:\n","            raise ValueError(f\"Word/phrase: {word} not a word or phrase\")\n","\n","\n","    def get_freq(self, word):\n","\n","        if word in self.stoi:\n","            return self.stoi.get(word)[1]\n","        else:\n","            raise ValueError(f\"Word/phrase: {word} not a word or phrase\")\n","\n","    def lookup_token(self, token):\n","\n","        if token in self.itos:\n","            return self.stoi.get(token)[0]\n","\n","        else:\n","\n","            raise ValueError(f\"out of index\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-02T10:46:01.648567Z","iopub.status.busy":"2023-10-02T10:46:01.648134Z","iopub.status.idle":"2023-10-02T10:46:01.659136Z","shell.execute_reply":"2023-10-02T10:46:01.658302Z","shell.execute_reply.started":"2023-10-02T10:46:01.648453Z"},"trusted":true},"outputs":[],"source":["# token generator generates tokens from the raw text after applying basic filters, removes punctuation.\n","def token_generator(iterator, tokenizer):\n","    r = re.compile('[a-z1-9]')\n","    for text in iterator:\n","\n","        res = tokenizer(text)\n","        res = list(filter(r.match, res))\n","        yield res\n","# this function generates tokens and creates a counts word frequency, using with vocab is created\n","def build_vocab(iterator, tokenizer, params, min_freq = 1, max_tokens=None):\n","    counter = Counter()\n","    for tokens in tqdm(token_generator(iterator, tokenizer)):\n","        counter.update(tokens)\n","        \n","    counter = sorted(counter.items(), key = lambda x:(-x[1], x[0]))\n","    word_freq = OrderedDict(counter)\n","    tokens = []\n","    \n","    for token, f in word_freq.items():\n","        if f >=min_freq:\n","            tokens.append((token, f))\n","            \n","    word_vocab = Vocab(tokens, specials=[params.SPECIALS, np.nan])\n","\n","    return word_vocab"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-02T10:46:01.664075Z","iopub.status.busy":"2023-10-02T10:46:01.663384Z","iopub.status.idle":"2023-10-02T10:46:01.675990Z","shell.execute_reply":"2023-10-02T10:46:01.675331Z","shell.execute_reply.started":"2023-10-02T10:46:01.664052Z"},"trusted":true},"outputs":[],"source":["# generates skipgrams as decribed in the paper, we have created rejection probabilities for words, so that words with high frequencies dont get trained much. \n","#Also provides collator function for dataset generation\n","class SkipGrams:\n","    def __init__(self, vocab, params, tokenizer):\n","        self.vocab = vocab\n","        self.params = params\n","        self.t = params.T\n","        self.tokenizer = tokenizer\n","        self.word_rejection_prob = self._create_rejection_prob()\n","        \n","    def _create_rejection_prob(self):\n","\n","        rejection_dict = {}\n","        for _, (word, freq) in self.vocab.stoi.items():\n","            rej_prob = 1-np.sqrt(\n","                    self.t / (freq/self.vocab.total_tokens + self.t)) # same as provided in paper\n","            rejection_dict[word] = rej_prob\n","        return rejection_dict\n","\n","    def collator(self, batch):\n","\n","        batch_input, batch_output = [], []\n","        r = re.compile('[a-z1-9]')\n","\n","        for text in batch:\n","            res = list(filter(r.match, self.tokenizer(text)))\n","            text_tokens = [self.vocab.get_index(x) for x in res]\n","            \n","            for idx in range(len(text_tokens)):\n","\n","                token_id_sequence = text_tokens[\n","                            max(idx - self.params.SKIPGRAM_N_WORDS +1 , 0) : idx] + text_tokens[idx+1 : idx + self.params.SKIPGRAM_N_WORDS]\n","\n","                prob = random.uniform(0.5, 1)\n","                target = text_tokens[idx]\n","\n","                if self.word_rejection_prob[target] >= prob:\n","                    continue\n","                    \n","                else:\n","                    for out_token in token_id_sequence:\n","                        prob = random.uniform(0.5, 1)\n","\n","                        if self.word_rejection_prob[out_token] >= prob:\n","                            continue\n","\n","                        else:\n","                            batch_input.append(target)\n","                            batch_output.append(out_token)\n","\n","        batch_input = torch.tensor(batch_input, dtype=torch.long)\n","        batch_output = torch.tensor(batch_output, dtype=torch.long)\n","\n","        return batch_input, batch_output\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-02T10:46:01.677981Z","iopub.status.busy":"2023-10-02T10:46:01.677049Z","iopub.status.idle":"2023-10-02T10:46:01.696297Z","shell.execute_reply":"2023-10-02T10:46:01.695334Z","shell.execute_reply.started":"2023-10-02T10:46:01.677930Z"},"trusted":true},"outputs":[],"source":["# created a negative sampling array using word frequencies to be used for sampling negative words. Frequencies are raised to the power of 0.75 as metioned in the paper.\n","class NegSampler:\n","    \n","    def __init__(self, vocab, ns_exponent, ns_array_length):\n","\n","        self.vocab = vocab\n","        self.ns_exponent = ns_exponent\n","        self.ns_array_length = ns_array_length\n","        self.ns_array =  self._create_negative_sampling()\n","        \n","        \n","    def __len__(self):\n","        return len(self.ns_array)\n","    \n","    \n","    def _create_negative_sampling(self):\n","\n","        word_freq_map = {word:freq**(self.ns_exponent) \\\n","                              for _,(word, freq) in\n","                              list(self.vocab.stoi.items())}\n","        word_freq_map = {\n","            word: max(1,int((freq/self.vocab.total_tokens)*self.ns_array_length))\n","            for word, freq in word_freq_map.items()\n","            }\n","        ns_array = []\n","        for word, freq in tqdm(word_freq_map.items()):\n","            ns_array = ns_array + [word]*freq\n","        return ns_array\n","    \n","    def sample(self, batch_size = 1, n_samples=1):\n","\n","        samples = []\n","        for _ in range(batch_size):\n","            samples.append(random.sample(self.ns_array, n_samples))\n","\n","        return torch.as_tensor(np.array(samples))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-02T10:46:01.698196Z","iopub.status.busy":"2023-10-02T10:46:01.697816Z","iopub.status.idle":"2023-10-02T10:46:01.963052Z","shell.execute_reply":"2023-10-02T10:46:01.961966Z","shell.execute_reply.started":"2023-10-02T10:46:01.698166Z"},"trusted":true},"outputs":[],"source":["from sklearn.preprocessing import normalize"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-02T10:46:01.965089Z","iopub.status.busy":"2023-10-02T10:46:01.964448Z","iopub.status.idle":"2023-10-02T10:46:01.978141Z","shell.execute_reply":"2023-10-02T10:46:01.976992Z","shell.execute_reply.started":"2023-10-02T10:46:01.965053Z"},"trusted":true},"outputs":[],"source":["class Model(nn.Module):\n","    def __init__(self, vocab, params) -> None:\n","        super().__init__()\n","        self.vocab = vocab\n","\n","        self.embeds =  nn.Embedding(self.vocab.__len__()+1,\n","                                          params.EMBED_DIM, max_norm=params.EMBED_MAX_NORM)\n","        self.context_embeds =  nn.Embedding(self.vocab.__len__()+1,\n","                                          params.EMBED_DIM, max_norm=params.EMBED_MAX_NORM)\n","        \n","    def forward (self, inputs, context):\n","\n","        target_embeddings = self.embeds(inputs)\n","        target_embeddings = torch.unsqueeze(target_embeddings, dim=1) # shape: Bx1xE\n","\n","        context_embeddings = self.context_embeds(context)\n","        context_embeddings = context_embeddings.permute(0, 2, 1) # B x E x (pos+neg size) transpose for batch multiplication\n","\n","        out = torch.bmm(target_embeddings, context_embeddings) # B x 1 x (pos+neg size)\n","        out = out.view(out.shape[0], out.shape[2])\n","\n","        return out\n","\n","    def normalize_embeddings(self):\n","        \n","        embeddings = list(self.embeds.parameters())[0]\n","        embeddings = embeddings.cpu().detach().numpy()\n","        self.norm_embeddings = normalize(embeddings)\n","\n","\n","    def get_similar_words(self, word, n=5):\n","\n","        embeddings = self.normalize_embeddings()\n","        word_ind = self.vocab.get_index(word)\n","\n","        dot_products = np.matmul(embeddings, embeddings[word_ind]).flatten()\n","        top_n = np.argsort(-dot_products)[1 : n + 1]\n","\n","        results = []\n","        for index in top_n:\n","            results.append(self.vocab.lookup_token(index))\n","\n","        return results\n","    \n","    def get_analogy(self, word, analogy, target_word, n=5):\n","        embeddings = self.normalize_embeddings()\n","        relation = embeddings[self.vocab.get_index(word)] - \\\n","        embeddings[self.vocab.get_index(analogy)] + embeddings[self.vocab.get_index(target_word)]\n","\n","        dot_products = np.matmul(embeddings, relation).flatten()\n","        top_n = np.argsort(-dot_products)[1 : n + 1]\n","\n","        results = []\n","        for index in top_n:\n","            results.append(self.vocab.lookup_token(index))\n","\n","\n","        return results"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-02T10:46:01.980551Z","iopub.status.busy":"2023-10-02T10:46:01.980082Z","iopub.status.idle":"2023-10-02T10:46:01.999311Z","shell.execute_reply":"2023-10-02T10:46:01.998372Z","shell.execute_reply.started":"2023-10-02T10:46:01.980513Z"},"trusted":true},"outputs":[],"source":["class Trainer:\n","\n","    def __init__(self, model, params, optimizer, vocab, skipgrams, negative_sampler,train_iter, valid_iter=None):\n","\n","        self.model = model\n","        self.optimizer = optimizer\n","        self.vocab = vocab\n","        self.train_iter = train_iter\n","        self.valid_iter = valid_iter\n","        self.skipgrams = skipgrams\n","        self.params = params\n","\n","        self.model.to(self.params.DEVICE)\n","        self.params.CRITERION.to(self.params.DEVICE)\n","\n","        self.negative_sampler = negative_sampler\n","\n","\n","\n","    def train_epoch(self):\n","\n","        self.model.train()\n","\n","        running_loss = []\n","\n","        for i, batch_data in tqdm(enumerate(self.train_loader), total=len(self.train_loader) ):\n","            \n","            if len(batch_data[0]) == 0:\n","                continue\n","                \n","            inputs = batch_data[0].to(params.DEVICE)\n","            pos_labels = batch_data[1].to(params.DEVICE)\n","            neg_labels = self.negative_sampler.sample(pos_labels.shape[0], self.params.NEG_SAMPLES)\n","\n","            neg_labels = neg_labels.to(self.params.DEVICE)\n","            context = torch.cat([pos_labels.view(pos_labels.shape[0], 1), neg_labels], dim=1)\n","\n","\n","            y_pos = torch.ones((pos_labels.shape[0], 1))\n","            y_neg = torch.zeros((neg_labels.shape[0], neg_labels.shape[1]))\n","\n","            y = torch.cat([y_pos, y_neg], dim=1).to(self.params.DEVICE)\n","\n","            self.optimizer.zero_grad()\n","\n","            outputs = self.model(inputs, context)\n","            loss = self.params.CRITERION(outputs, y)\n","\n","            loss.backward()\n","            self.optimizer.step()\n","\n","            running_loss.append(loss.item())\n","\n","        epoch_loss = np.mean(running_loss)\n","\n","        return epoch_loss\n","\n","    def validate_epoch(self):\n","\n","        self.model.eval()\n","        running_loss = []\n","\n","        with torch.no_grad():\n","            \n","            for i, batch_data in enumerate(self.val_loader):\n","                if len(batch_data[0]) == 0:\n","                    continue\n","\n","                inputs = batch_data[0].to(params.DEVICE)\n","                pos_labels = batch_data[1].to(params.DEVICE)\n","                neg_labels = self.negative_sampler.sample(pos_labels.shape[0], self.params.NEG_SAMPLES)\n","\n","                neg_labels = neg_labels.to(self.params.DEVICE)\n","                context = torch.cat([pos_labels.view(pos_labels.shape[0], 1), neg_labels], dim=1)\n","\n","\n","                y_pos = torch.ones((pos_labels.shape[0], 1))\n","                y_neg = torch.zeros((neg_labels.shape[0], neg_labels.shape[1]))\n","\n","                y = torch.cat([y_pos, y_neg], dim=1).to(self.params.DEVICE)\n","\n","                outputs = self.model(inputs, context)\n","                loss = self.params.CRITERION(outputs, y)\n","\n","                running_loss.append(loss.item())\n","\n","        return np.mean(running_loss)\n","\n","\n","    def train(self):\n","\n","        self.train_loader = DataLoader(\n","            self.train_iter,\n","            batch_size=self.params.BATCH_SIZE,\n","            shuffle=True,\n","            collate_fn=self.skipgrams.collator)\n","\n","        if self.valid_iter is not None:\n","\n","            self.val_loader = DataLoader(\n","                self.valid_iter,\n","                batch_size=self.params.BATCH_SIZE,\n","                shuffle=True,\n","                collate_fn=self.skipgrams.collator)\n","\n","        for epoch in range(self.params.N_EPOCHS):\n","            train_loss = self.train_epoch()\n","            print(f\"\"\" EPOCH: {epoch+1}, Train Loss: {train_loss}\"\"\")\n","                \n","            if self.valid_iter is not None:\n","                val_loss = self.validate_epoch()\n","                print(f\"Validaton loss:{val_loss}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-02T10:46:02.001659Z","iopub.status.busy":"2023-10-02T10:46:02.000976Z","iopub.status.idle":"2023-10-02T10:46:59.543829Z","shell.execute_reply":"2023-10-02T10:46:59.542831Z","shell.execute_reply.started":"2023-10-02T10:46:02.001627Z"},"trusted":true},"outputs":[],"source":["params = Word2VecParams()\n","train_iter, valid_iter = get_data()\n","tokenizer = get_tokenizer(params.TOKENIZER)\n","vocab = build_vocab(train_iter+valid_iter, tokenizer, params)\n","skip_grams = SkipGrams(vocab, params, tokenizer)\n","negative_smapler = NegSampler(vocab, ns_exponent=0.75, ns_array_length=params.NS_ARRAY_LEN)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-02T10:46:59.546050Z","iopub.status.busy":"2023-10-02T10:46:59.545153Z","iopub.status.idle":"2023-10-02T10:46:59.594889Z","shell.execute_reply":"2023-10-02T10:46:59.593906Z","shell.execute_reply.started":"2023-10-02T10:46:59.546014Z"},"trusted":true},"outputs":[],"source":["model = Model(vocab, params)\n","if params.DEVICE == 'cuda':\n","    model.cuda()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-02T15:43:22.753745Z","iopub.status.busy":"2023-10-02T15:43:22.752984Z","iopub.status.idle":"2023-10-02T15:43:23.003109Z","shell.execute_reply":"2023-10-02T15:43:23.001810Z","shell.execute_reply.started":"2023-10-02T15:43:22.753711Z"},"trusted":true},"outputs":[],"source":["optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-02T10:46:59.604094Z","iopub.status.busy":"2023-10-02T10:46:59.603581Z","iopub.status.idle":"2023-10-02T10:48:17.411501Z","shell.execute_reply":"2023-10-02T10:48:17.410012Z","shell.execute_reply.started":"2023-10-02T10:46:59.604064Z"},"trusted":true},"outputs":[],"source":["trainer = Trainer(\n","        model=model,\n","        params=params,\n","        optimizer=optimizer,\n","        train_iter=train_iter+valid_iter,\n","        vocab=vocab,\n","        skipgrams=skip_grams,\n","        negative_sampler= negative_smapler)\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-02T10:49:35.768164Z","iopub.status.busy":"2023-10-02T10:49:35.767743Z","iopub.status.idle":"2023-10-02T10:49:35.773076Z","shell.execute_reply":"2023-10-02T10:49:35.772139Z","shell.execute_reply.started":"2023-10-02T10:49:35.768136Z"},"trusted":true},"outputs":[],"source":["ver = 2"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-02T10:49:46.220862Z","iopub.status.busy":"2023-10-02T10:49:46.220519Z","iopub.status.idle":"2023-10-02T10:49:46.239393Z","shell.execute_reply":"2023-10-02T10:49:46.238402Z","shell.execute_reply.started":"2023-10-02T10:49:46.220835Z"},"trusted":true},"outputs":[],"source":["torch.save(model.state_dict(), f\"word2vec_{ver}.pth\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
